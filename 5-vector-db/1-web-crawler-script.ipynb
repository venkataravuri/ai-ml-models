{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrape Script\n",
    "\n",
    "This colab notebook scrapes a website and extracts data.\n",
    "- Scrapes website using sitemap.\n",
    "- Extract page content and remove HTML tags, JavaScript and others.\n",
    "- Create text chunks from page content paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install chromium and chromium-driver\n",
    "\n",
    "%%shell\n",
    "# apt-get -qq update\n",
    "apt-get -qq install chromium chromium-driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necessary python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q  selenium requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Chrome web driver to crawl website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(executable_path=r'/usr/bin/chromedriver')\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "user_agent = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.517 Safari/537.36'\n",
    "options.add_argument('user-agent={0}'.format(user_agent))\n",
    "browser = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Crawl [ElevenLabs Blog](https://elevenlabs.io/blog/).\n",
    "- Extract latest 10 blog articles content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = 'elevenlabs.io'\n",
    "websit_url = 'https://' + website + '/blog'\n",
    "sitemap = websit_url + '/sitemap-posts.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_linked_pages(soup):\n",
    "  links = []\n",
    "  for tag in soup.find_all(['a']):\n",
    "    if tag.get('href') not in ['#', '/', '', None]:\n",
    "      links.append(tag.get('href'))\n",
    "\n",
    "  filtered = filter(lambda link:website in link, links)\n",
    "\n",
    "  return list(dict.fromkeys(list(filtered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soup_by_url(url):\n",
    "  browser.get(url)\n",
    "  # Selenium hands the page source to Beautiful Soup\n",
    "  return BeautifulSoup(browser.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page_content(soup):\n",
    "  results = soup.find_all(['h1', 'p', 'ul', 'li', 'pre', 'button'])\n",
    "  page_text = [result.text for result in results]\n",
    "  return ' '.join(page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = soup_by_url(sitemap)\n",
    "links = find_linked_pages(soup)\n",
    "print(f\"Linked pages: {links}\")\n",
    "documents = []\n",
    "for link in links:\n",
    "  soup = soup_by_url(link)\n",
    "  page_content = extract_page_content(soup)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
