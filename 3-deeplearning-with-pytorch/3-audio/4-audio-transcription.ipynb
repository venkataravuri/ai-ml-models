{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNUcuwLV0kJ2o8/cfRau1Nb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["https://deepgram.com/learn/python-speech-recognition-locally-torchaudio"],"metadata":{"id":"RQmLOYmhcQVW"}},{"cell_type":"code","source":["import torch\n","import torch.multiprocessing as mp\n","import torchaudio\n","from torchaudio.io import StreamReader"],"metadata":{"id":"i-0Pam6tcY2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ITERATIONS = 100\n","\n","def stream(queue: mp.Queue(),\n","           format: str,\n","           src: str,\n","           frames_per_chunk: int,\n","           sample_rate: int):\n","  ''' Streams audio data\n","\n","  Parameters:\n","    queue: Queue of data chunks\n","    format: Format\n","    src: Source\n","    frames_per_chunk: How many frames are in each data chunk\n","    sample_rate: Sample rate\n","\n","  Returns:\n","    None'''\n","\n","  print(\"Initializing Audio Stream\")\n","  streamer = StreamReader(src, format=format)\n","\n","  streamer.add_basic_audio_stream(frames_per_chunk=frames_per_chunk, sample_rate=sample_rate)\n","  print(\"Streaming\\n\")\n","  stream_iterator = streamer.stream(timeout=-1, backoff=1.0)\n","  for _ in range(ITERATIONS):\n","    (chunk,) = next(stream_iterator)\n","    queue.put(chunk)"],"metadata":{"id":"VLpTGZUzczyP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class InferencePipeline:\n","  '''Creates an inference pipeline for streaming audio data'''\n","  def __init__(self,\n","               pipeline: torchaudio.pipelines.RNNTBundle,\n","               beam_width: int=10):\n","    self.pipeline = pipeline\n","    self.feature_extractor = pipeline.get_streaming_feature_extractor()\n","    self.decoder = pipeline.get_decoder()\n","    self.token_processor = pipeline.get_token_processor()\n","    self.beam_width = beam_width\n","    self.state = None\n","    self.hypothesis = None\n","\n","  def infer(self, segment: torch.Tensor) -> str:\n","    features, length = self.feature_extractor(segment)\n","    predictions, self.state = self.decoder.infer(features, length, self.beam_width, state=self.state, hypothesis=self.hypothesis)\n","    self.hypothesis = predictions[0]\n","    transcript = self.token_processor(self.hypothesis[0], lstrip=False)\n","    return transcript"],"metadata":{"id":"Vg1GCrUMew5V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ContextCacher:\n","  def __init__(self, segment_length: int, context_length: int):\n","    self.segment_length = segment_length\n","    self.context_length = context_length\n","    self.context = torch.zeros([context_length])\n","\n","  def __call__(self, chunk: torch.Tensor):\n","    if chunk.size(0) < self.segment_length:\n","      chunk = torch.nn.functional.pad(chunk, (0, self.segment_length - chunk.size(0)))\n","    chunk_with_context = torch.cat((self.context, chunk))\n","    self.context = chunk[-self.context_length:]\n","    return chunk_with_context"],"metadata":{"id":"RB7FMWqtgdHH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main(device: str, src: str, bundle: torchaudio.pipelines):\n","  pipeline = InferencePipeline(bundle)\n","  sample_rate = bundle.sample_rate\n","  segment_length = bundle.segment_length * bundle.hop_length\n","  context_length = bundle.right_context_length * bundle.hop_length\n","\n","  cacher = ContextCacher(segment_length, context_length)\n","\n","  @torch.inference_mode()\n","  def infer():\n","    for _ in range(ITERATIONS):\n","      chunk = q.get()\n","      segment = cacher(chunk[:, 0])\n","      transcript = pipeline.infer(segment)\n","      print(transcript, end=\"\", flush=True)\n","\n","\n","  ctx = mp.get_context(\"spawn\")\n","  q = ctx.Queue()\n","  p = ctx.Process(target=stream, args=(q, device, src, segment_length, sample_rate))\n","  p.start()\n","  infer()\n","  p.join()\n","\n","if __name__ == \"__main__\":\n","  main(\n","      device=\"avfoundation\",\n","      src=\":1\",\n","      bundle=torchaudio.pipelines.EMFORMER_RNNT_BASE_LIBRISPEECH\n","  )"],"metadata":{"id":"X6omDld0hmVA"},"execution_count":null,"outputs":[]}]}