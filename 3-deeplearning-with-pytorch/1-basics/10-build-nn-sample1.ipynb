{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([7])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Linear weights: Parameter containing:\n",
      "tensor([[ 0.0194, -0.0270, -0.0115,  ...,  0.0004,  0.0002, -0.0165],\n",
      "        [ 0.0279,  0.0348,  0.0092,  ...,  0.0270,  0.0071, -0.0130],\n",
      "        [ 0.0215,  0.0118, -0.0100,  ..., -0.0033, -0.0173,  0.0179],\n",
      "        ...,\n",
      "        [-0.0274, -0.0195,  0.0099,  ...,  0.0280,  0.0345, -0.0177],\n",
      "        [-0.0148, -0.0115, -0.0095,  ...,  0.0107,  0.0308,  0.0298],\n",
      "        [-0.0352,  0.0059,  0.0068,  ..., -0.0061,  0.0183, -0.0301]],\n",
      "       requires_grad=True)\n",
      "First Linear bias: Parameter containing:\n",
      "tensor([ 8.5315e-03,  1.2039e-02,  3.3523e-02,  6.7885e-03, -3.1905e-02,\n",
      "         1.2577e-02,  2.8674e-02, -2.1631e-02,  3.4123e-02,  2.0768e-02,\n",
      "        -7.6371e-03,  5.2883e-03,  2.3816e-02, -1.8611e-02, -9.9586e-03,\n",
      "         1.0200e-02, -2.6447e-02, -2.6226e-02, -2.8981e-02, -1.9178e-02,\n",
      "         3.3051e-02,  3.3091e-02, -3.4258e-02,  2.4287e-02, -3.3330e-02,\n",
      "        -1.7082e-02, -2.4185e-02,  3.0944e-02, -3.4343e-02, -5.1196e-03,\n",
      "         2.0588e-02, -6.8013e-03,  2.7161e-02, -3.4835e-02, -1.4119e-02,\n",
      "        -2.2440e-02,  2.5262e-04,  1.9471e-02, -2.6415e-02, -2.6293e-02,\n",
      "         2.3156e-02,  2.7029e-02, -2.6073e-02, -6.1027e-03, -1.5391e-03,\n",
      "        -7.4494e-03, -2.8382e-03, -3.1984e-02,  1.3100e-02,  2.8419e-02,\n",
      "         2.1810e-03, -3.0306e-02, -5.5294e-03,  1.4720e-02, -2.4275e-02,\n",
      "         3.3905e-02,  1.6788e-02,  1.6447e-02, -1.5259e-02, -2.1104e-02,\n",
      "        -1.1675e-02,  2.5859e-02,  3.2423e-02,  2.8365e-02, -1.4047e-02,\n",
      "        -1.7520e-02, -3.8950e-03, -1.1624e-02, -7.7199e-03,  3.9674e-03,\n",
      "         6.8901e-03,  3.3188e-03, -8.5134e-03,  1.2475e-02,  1.4630e-02,\n",
      "        -3.2040e-04, -5.4261e-03,  3.3554e-02, -1.1845e-02, -1.7489e-03,\n",
      "        -2.8282e-02, -1.3996e-02,  2.3457e-02,  4.5074e-03, -1.9780e-02,\n",
      "         2.8863e-02,  1.3055e-02,  2.5932e-05,  3.5387e-02,  1.0705e-02,\n",
      "         1.3396e-03,  3.3787e-02, -1.2833e-02,  1.7793e-02,  1.3694e-02,\n",
      "         3.1725e-02, -2.2824e-02,  2.1973e-02, -1.1051e-03,  2.3973e-02,\n",
      "         2.8093e-02, -2.8193e-02,  2.6163e-02, -4.5548e-03,  5.0189e-03,\n",
      "        -2.9221e-03, -2.3878e-02,  1.7489e-02, -1.6922e-03,  2.8585e-02,\n",
      "        -1.5733e-02, -2.6756e-02, -1.0158e-02,  3.0887e-02, -2.0183e-02,\n",
      "         2.2457e-02,  3.2478e-02,  2.5523e-02, -8.3421e-04, -3.2835e-02,\n",
      "        -1.1559e-02,  1.8151e-03, -1.5568e-02,  1.3236e-02,  3.0518e-02,\n",
      "        -2.1116e-02,  3.3608e-02,  2.8771e-02,  8.2303e-03,  2.8943e-02,\n",
      "         9.9530e-03,  1.7101e-02, -1.1118e-02,  6.3541e-03,  2.8490e-02,\n",
      "        -1.3155e-02,  3.4834e-02,  2.8067e-02, -5.9295e-03,  1.6212e-02,\n",
      "         1.0960e-05, -2.2897e-02, -8.7931e-03, -2.9706e-02,  1.7922e-02,\n",
      "         7.3405e-04,  7.3220e-03, -1.8432e-02, -2.4953e-02, -1.2119e-02,\n",
      "        -2.9390e-02, -2.1526e-02,  1.3094e-02, -5.5843e-03,  1.0439e-02,\n",
      "        -8.9648e-03,  3.0698e-02,  1.2354e-02,  3.2532e-02, -1.0396e-02,\n",
      "        -6.9283e-03,  4.1234e-04, -1.7531e-02, -1.4644e-02,  3.5613e-02,\n",
      "        -2.7324e-03,  6.5096e-03,  3.0822e-02, -5.8191e-03, -2.7125e-02,\n",
      "        -1.0629e-02, -1.5286e-03, -2.7308e-02, -1.0466e-02,  1.4128e-02,\n",
      "        -1.8520e-02,  1.0930e-03,  4.6640e-03,  9.4876e-03,  1.3036e-03,\n",
      "         1.0100e-02, -2.2813e-02, -2.5159e-02,  3.2400e-02, -1.4479e-02,\n",
      "         1.2201e-02, -1.8460e-02,  2.7930e-02,  2.3850e-02,  1.7357e-02,\n",
      "         1.7343e-02, -2.5119e-02, -4.8288e-03,  3.1734e-03,  2.1862e-02,\n",
      "         2.2988e-02,  6.1351e-03, -1.0142e-02,  8.1575e-03,  5.9892e-03,\n",
      "         3.5317e-02,  1.1288e-02,  2.8989e-02,  5.3494e-03, -1.2049e-02,\n",
      "        -1.7088e-02, -3.4146e-02, -7.0903e-03, -8.3917e-03, -2.3760e-03,\n",
      "         8.6761e-03, -2.0715e-02, -8.8596e-04, -1.6102e-02, -1.9712e-02,\n",
      "         2.3921e-02, -1.9042e-02,  6.4296e-03,  2.4131e-02,  9.8016e-03,\n",
      "         2.5824e-02,  3.3096e-02,  8.6755e-03, -2.9006e-03, -1.5263e-02,\n",
      "         2.7397e-02,  3.1084e-02, -1.5028e-02,  2.1374e-02, -6.0706e-03,\n",
      "        -3.2440e-02,  1.4966e-02,  2.8834e-02,  2.0740e-02, -3.9055e-03,\n",
      "         2.1110e-02,  3.2973e-02,  2.6685e-02, -4.8663e-03,  1.6625e-02,\n",
      "        -1.8196e-02,  3.2843e-02, -1.8073e-02,  2.2644e-02, -2.4125e-02,\n",
      "        -1.7691e-02,  3.1225e-02,  2.2014e-02,  3.4304e-02,  8.5571e-03,\n",
      "         2.1028e-02, -1.0664e-02, -3.0233e-03, -3.0874e-02, -3.5530e-02,\n",
      "        -1.1257e-02,  8.0747e-03,  1.6475e-02,  3.4096e-02,  1.9101e-02,\n",
      "        -1.1790e-02,  6.6664e-03,  3.2678e-02,  2.9659e-03, -5.7243e-03,\n",
      "         2.3629e-02, -2.2959e-02,  1.5792e-02, -1.5395e-02, -1.8528e-02,\n",
      "        -1.0680e-02,  2.1196e-02,  1.9534e-02, -4.6889e-03, -7.4126e-03,\n",
      "         2.0243e-02,  4.2371e-03,  3.2157e-02, -2.3617e-02, -2.5186e-02,\n",
      "         3.2268e-02, -3.0545e-02, -9.3445e-03, -1.9319e-02, -2.6424e-02,\n",
      "        -5.8054e-03,  1.9791e-02, -1.6499e-02, -3.9376e-04,  1.1325e-02,\n",
      "         1.3312e-02, -3.1092e-02,  3.1904e-02,  7.1925e-03, -2.0088e-02,\n",
      "         2.6417e-02, -2.6082e-02, -1.6428e-02, -4.9280e-03, -3.4738e-02,\n",
      "        -1.6914e-02, -1.4859e-03, -3.4292e-03,  1.6631e-02,  1.7867e-02,\n",
      "         1.0295e-02,  2.6329e-02, -3.0415e-02,  1.9324e-02,  9.7237e-03,\n",
      "         2.5908e-02,  6.7313e-03, -3.0191e-02,  6.7507e-03, -2.8543e-02,\n",
      "        -1.9444e-03, -3.1999e-02, -1.0856e-02, -5.9179e-03, -2.2381e-02,\n",
      "        -1.4524e-02,  1.2159e-02, -1.0398e-02, -6.0529e-03, -1.7770e-02,\n",
      "        -5.8998e-04, -3.3906e-02, -3.3606e-02, -5.8235e-03,  2.4801e-02,\n",
      "         1.7453e-02,  7.6634e-03, -3.3967e-02, -3.3974e-02,  4.1638e-03,\n",
      "         1.0384e-02,  3.4581e-02, -2.3547e-02,  2.3875e-02, -1.5697e-02,\n",
      "        -7.0306e-03,  3.4305e-02, -1.1323e-02,  1.8691e-02,  2.0277e-02,\n",
      "         1.1608e-02,  2.1987e-02, -1.0433e-02, -6.3947e-04, -1.8318e-02,\n",
      "        -1.9230e-02, -2.6186e-02,  3.2610e-02, -2.6583e-02, -1.0051e-02,\n",
      "         5.5935e-03,  1.8646e-02, -2.4552e-02,  2.4621e-02,  1.3562e-02,\n",
      "         3.2245e-02, -3.4743e-02,  1.8264e-02,  3.9908e-04,  9.2724e-04,\n",
      "         6.1446e-03,  1.0851e-02, -2.8382e-02,  1.9962e-02, -1.6565e-03,\n",
      "        -1.2540e-02, -2.5776e-02,  2.5540e-02, -9.6978e-03,  1.2368e-02,\n",
      "        -3.3072e-02,  1.8969e-02,  1.9466e-03,  2.5261e-02, -1.7075e-02,\n",
      "        -2.5838e-02,  2.2222e-02,  2.1394e-02, -2.3748e-02, -1.1993e-02,\n",
      "        -2.9199e-02,  2.4948e-02,  1.0998e-02, -1.3783e-02, -1.9856e-05,\n",
      "        -3.0693e-03, -1.5708e-02, -1.0825e-03, -2.2164e-02,  8.6692e-03,\n",
      "        -3.1000e-02,  3.7181e-03,  1.8694e-02, -3.1699e-03, -3.3229e-02,\n",
      "         3.3521e-02, -1.8644e-02, -2.8289e-02,  2.6833e-02,  1.0114e-02,\n",
      "         2.9222e-02, -1.0762e-02,  3.3090e-04, -3.0748e-02, -1.9016e-02,\n",
      "         4.3995e-03,  5.6263e-03, -2.8163e-02, -3.3681e-02,  3.3966e-02,\n",
      "         7.6868e-03,  1.1579e-02,  1.9342e-03,  2.0733e-02,  3.5472e-03,\n",
      "         1.1599e-02, -3.4872e-02,  1.5040e-02, -2.7639e-02, -3.2339e-02,\n",
      "         1.3522e-02,  2.0798e-02,  1.0918e-02,  1.1718e-03, -1.9927e-02,\n",
      "        -2.7034e-02,  3.0294e-02, -1.1296e-02,  2.0906e-02,  1.5461e-02,\n",
      "         2.7019e-02, -3.2770e-02, -1.0729e-02,  1.7096e-02,  3.3791e-03,\n",
      "        -1.2569e-02,  2.4036e-02,  9.3041e-03,  2.3246e-02, -4.6927e-03,\n",
      "         2.0457e-02, -2.8962e-03, -1.9673e-02, -3.4580e-02, -2.0675e-02,\n",
      "         1.9809e-03,  2.0496e-02, -1.6680e-02, -3.2991e-02, -1.6878e-02,\n",
      "         1.7079e-02, -2.3143e-03,  1.1011e-02,  3.3853e-02,  2.6939e-02,\n",
      "         3.3752e-02,  2.4982e-02,  3.0453e-02, -7.9259e-03, -2.8389e-02,\n",
      "         3.3014e-02, -4.1718e-03,  9.4398e-03,  4.0793e-03, -7.3730e-03,\n",
      "        -3.2653e-02, -2.6128e-02, -2.7914e-02, -1.9468e-02,  2.6832e-02,\n",
      "        -9.4298e-03,  2.7226e-02, -7.1142e-03,  3.8181e-03, -1.4679e-02,\n",
      "        -9.6812e-03,  2.4141e-02,  1.5097e-02,  2.6089e-02,  2.8861e-02,\n",
      "         2.2566e-03,  1.7489e-02,  9.3457e-04, -3.3886e-02, -2.8241e-02,\n",
      "         8.8176e-03, -5.2292e-03,  3.3117e-02,  5.3128e-03, -3.0996e-02,\n",
      "        -3.0541e-02,  8.3733e-03, -8.4584e-03,  2.0740e-02, -1.0957e-02,\n",
      "         1.7213e-02, -2.7409e-02, -1.7344e-02, -7.9167e-04,  1.7795e-02,\n",
      "        -2.2455e-02,  4.9573e-03, -2.5112e-02, -2.9748e-02,  1.0338e-02,\n",
      "         1.1530e-02,  2.1165e-02], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(f\"First Linear weights: {model.linear_relu_stack[0].weight}\")\n",
    "print(f\"First Linear bias: {model.linear_relu_stack[0].bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.1268, -0.4109,  0.1026,  0.0354, -0.3961,  0.9708, -1.0732, -0.1553,\n",
      "         -0.3270, -0.0691,  0.4274, -0.7179, -0.2249,  0.3753,  0.1114, -0.0109,\n",
      "          0.2810,  0.0681,  0.3944,  0.7126],\n",
      "        [ 0.1987, -0.3817,  0.4069,  0.1651, -0.1994,  0.7672, -1.1217, -0.0476,\n",
      "         -0.1003, -0.1507, -0.1249, -0.4749, -0.1602,  0.1294,  0.1653, -0.2578,\n",
      "         -0.0104, -0.1848,  0.5645, -0.2163],\n",
      "        [-0.0063, -0.1533,  0.1555,  0.1017, -0.2186,  0.6161, -0.9855, -0.0180,\n",
      "         -0.1548, -0.0783,  0.3516, -0.5128,  0.2949,  0.3037,  0.0465, -0.3492,\n",
      "          0.1735, -0.1883,  0.8476, -0.1377]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.0000, 0.1026, 0.0354, 0.0000, 0.9708, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.4274, 0.0000, 0.0000, 0.3753, 0.1114, 0.0000, 0.2810, 0.0681,\n",
      "         0.3944, 0.7126],\n",
      "        [0.1987, 0.0000, 0.4069, 0.1651, 0.0000, 0.7672, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1294, 0.1653, 0.0000, 0.0000, 0.0000,\n",
      "         0.5645, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1555, 0.1017, 0.0000, 0.6161, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.3516, 0.0000, 0.2949, 0.3037, 0.0465, 0.0000, 0.1735, 0.0000,\n",
      "         0.8476, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.2053e-01,  1.2883e-01,  2.4502e-01,  5.6018e-02, -3.8854e-02,\n",
      "         -1.4776e-01,  2.2961e-01,  2.0153e-01, -1.7247e-01,  8.0072e-03],\n",
      "        [-2.3116e-01,  2.4106e-01,  2.9364e-01,  2.0444e-04,  1.0850e-02,\n",
      "         -1.4845e-01,  2.3614e-01,  1.5570e-01, -1.3716e-01, -2.5578e-02],\n",
      "        [-3.7269e-01,  1.4558e-01,  3.1490e-01, -5.0411e-02, -4.6698e-02,\n",
      "         -1.1032e-01,  3.7456e-01,  2.5320e-01, -2.5225e-01, -2.2954e-02]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size:torch.Size([512, 784]) | Values: tensor([[ 0.0194, -0.0270, -0.0115,  ...,  0.0004,  0.0002, -0.0165],\n",
      "        [ 0.0279,  0.0348,  0.0092,  ...,  0.0270,  0.0071, -0.0130]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Layer: linear_relu_stack.0.bias | Size:torch.Size([512]) | Values: tensor([0.0085, 0.0120], grad_fn=<SliceBackward0>)\n",
      "Layer: linear_relu_stack.2.weight | Size:torch.Size([512, 512]) | Values: tensor([[ 0.0145, -0.0125, -0.0072,  ..., -0.0199, -0.0113, -0.0090],\n",
      "        [ 0.0204,  0.0095,  0.0172,  ...,  0.0250,  0.0176,  0.0313]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Layer: linear_relu_stack.2.bias | Size:torch.Size([512]) | Values: tensor([-0.0089, -0.0381], grad_fn=<SliceBackward0>)\n",
      "Layer: linear_relu_stack.4.weight | Size:torch.Size([10, 512]) | Values: tensor([[-0.0334,  0.0399,  0.0298,  ...,  0.0084,  0.0282, -0.0059],\n",
      "        [ 0.0062,  0.0303,  0.0240,  ..., -0.0196,  0.0362,  0.0275]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Layer: linear_relu_stack.4.bias | Size:torch.Size([10]) | Values: tensor([0.0332, 0.0053], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size:{param.size()} | Values: {param[:2]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
